{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"dataset.json.txt\") as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jump_start', 'reminder', 'whisper_mode', 'what_are_your_hobbies', 'tire_pressure', 'vaccines', 'reset_settings', 'sync_device', 'time', 'timezone', 'travel_notification', 'update_playlist', 'balance', 'mpg', 'w2', 'restaurant_suggestion', 'book_flight', 'who_made_you', 'change_ai_name', 'calculator']\n"
     ]
    }
   ],
   "source": [
    "intent_classes=[]\n",
    "unique_intent=[]\n",
    "vocab=[]\n",
    "\n",
    "#print(data.keys())\n",
    "for i in data[\"train\"]:\n",
    "        if i[1] not in intent_classes:\n",
    "            intent_classes.append(i[1])\n",
    "            \n",
    "unique_intent=random.sample(intent_classes, 20)\n",
    "print(unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent=[]\n",
    "sentences=[]\n",
    "intent_val=[]\n",
    "sentences_val=[]\n",
    "intent_test=[]\n",
    "sentences_test=[]\n",
    "\n",
    "for i in data[\"train\"]:\n",
    "    if i[1] in unique_intent:\n",
    "        intent.append(i[1])\n",
    "        sentences.append(i[0])\n",
    "\n",
    "for i in data[\"val\"]:\n",
    "    if i[1] in unique_intent:\n",
    "        intent_val.append(i[1])\n",
    "        sentences_val.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "max_length=0\n",
    "\n",
    "def cleaning(sentences):\n",
    "    words=[]\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([stemmer.stem(i.lower()) for i in w])\n",
    "    \n",
    "    return words\n",
    "\n",
    "cleaned_words=cleaning(sentences)\n",
    "cleaned_words_val=cleaning(sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461\n"
     ]
    }
   ],
   "source": [
    "for sent in cleaned_words:\n",
    "    for i in sent:\n",
    "        if i not in vocab:\n",
    "            vocab.append(i)\n",
    "\n",
    "for sent in cleaned_words_val:\n",
    "    for i in sent:\n",
    "        if i not in vocab:\n",
    "            vocab.append(i)\n",
    "\n",
    "vocab.sort()\n",
    "n=len(vocab)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1461)\n"
     ]
    }
   ],
   "source": [
    "train_X,val_X=[],[]\n",
    "\n",
    "for sent in cleaned_words:\n",
    "    max_length=max(max_length,len(sent))\n",
    "    list1=[0]*n\n",
    "    for word in sent:\n",
    "        index=vocab.index(word)\n",
    "        list1[index]=1\n",
    "    train_X.append(list1)\n",
    "    \n",
    "Train_X=np.array(train_X)\n",
    "print(Train_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1461)\n"
     ]
    }
   ],
   "source": [
    "for sent in cleaned_words_val:\n",
    "    #max_length=max(max_length,len(sent))\n",
    "    list1=[0]*n\n",
    "    for word in sent:\n",
    "        index=vocab.index(word)\n",
    "        list1[index]=1\n",
    "    val_X.append(list1)\n",
    "    \n",
    "Val_X=np.array(val_X)\n",
    "print(Val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y,val_Y=[],[]\n",
    "for val in intent:\n",
    "    list1=[0]*len(unique_intent)\n",
    "    index=unique_intent.index(val)\n",
    "    list1[index]=1\n",
    "    train_Y.append(list1)\n",
    "train_Y=np.array(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in intent_val:\n",
    "    list1=[0]*len(unique_intent)\n",
    "    index=unique_intent.index(val)\n",
    "    list1[index]=1\n",
    "    val_Y.append(list1)\n",
    "\n",
    "val_Y=np.array(val_Y)\n",
    "\n",
    "max_features=Train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_features, max_length):\n",
    "   # Input for variable-length sequences of integers\n",
    "   inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "   # Embed each integer in a 128-dimensional vector\n",
    "   x = layers.Embedding(max_features, 128)(inputs)\n",
    "\n",
    "   x = layers.Bidirectional(layers.LSTM(128))(x)\n",
    "# Add a classifier\n",
    "   outputs = layers.Dense(32, activation=\"relu\")(x)\n",
    "   outputs=  layers.Dropout(0.5)\n",
    "   outputs=  layers.Dense(20, activation=\"softmax\")(x)\n",
    "   model = keras.Model(inputs, outputs)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         187008    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 455,316\n",
      "Trainable params: 455,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_features, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      " 608/2000 [========>.....................] - ETA: 15:25 - loss: 3.0057 - accuracy: 0.0493"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "hist=model.fit(Train_X, train_Y , batch_size=32, epochs=2, validation_data=(Val_X,val_Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
